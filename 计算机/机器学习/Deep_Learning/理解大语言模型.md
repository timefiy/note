# 理解大语言模型

- LLM：大预言模型
	- 当我们说语言模型“理解”时，并不是说它们具有人类的意识或理解能力，而是指它们能够以看起来连贯且符合上下文的方式处理和生成文本。
	- 深度学习是机器学习和人工智能 (AI) 的一个子集，主要关注神经网络，LLM 可以基于深度学习理论在海量文本数据上进行训练
	- LLM 的成功可以归因于支撑 LLM 的 Transformer 架构，以及 LLM 训练所用的海量数据。
- NLP： 自然语言处理

## LLM是什么

LLM（大语言模型）是一个旨在理解、生成和响应人类文本的神经网络。这些模型是深度神经网络，在海量文本数据上训练，基本涵盖了互联网上大部分公开可用的文本数据集。
由于 LLM 能够生成文本，因此它们通常被称为一种生成式人工智能 (AI,GenAI)

![](assets/理解大语言模型/file-20260228202002665.png)

用于实现人工智能的==算法是机器学习领域的核心==.机器学习往往不需要明确的编程实现，而是涉及可以从数据中学习并基于数据做出预测或决策的算法研究
如图 1.1 所示，==深度学习是机器学习的一个子集==，专注于使用三层或更多层的神经网络（即深度神经网络）来建模数据中的复杂模式和抽象。与深度学习不同，传统机器学习需要手动提取特征。这意味着人类专家需要识别并选择最相关的特征供模型使用。
在传统机器学习中，人类专家会手动提取电子邮件文本中的特征，例如某些触发词的频率（“奖品”、“获胜”、“免费”）、感叹号的数量、全大写单词的使用，或者是否存在可疑链接。基于这些专家定义的特征创建的数据集随后用于训练模型。与传统机器学习不同，深度学习不需要手动提取特征，这意味着人类专家不需要为深度学习模型识别和选择最相关的特征。（不过，无论是在传统机器学习还是深度学习的垃圾邮件分类中，仍然需要收集标签，如垃圾邮件或非垃圾邮件，而这些标签需要由专家或用户进行收集。）

## LLM的应用
![](assets/理解大语言模型/file-20260228202748099.png)
LLM 被广泛用于机器翻译、新文本生成、情感分析、文本摘要等多种任务。
此外，LLM 还可以有效地从医学或法律等专业领域的大量文本中检索知识。这包括筛选文档、总结长段落以及回答技术性问题
总之，LLM 在自动化几乎所有涉及文本解析和生成的任务中都是不可或缺的。

## 构建和使用LLM的步骤
研究表明，在建模性能方面，专为特定任务或领域定制的 LLM 通常能超过通用的 LLM，比如 ChatGPT，这些通用模型设计用于多种应用场景。
创建 LLM 的一般过程包括**预训练**和**微调**。术语 "pre" 在 "pretraining"（预训练） 中指的是初始阶段，此时模型（如 LLM）在一个大型且多样化的数据集上进行训练，以便获得对语言的广泛理解。预训练模型随后作为基础资源，可以通过微调进一步优化。微调是指模型在一个更针对特定任务或领域的数据集上进行专门训练。[^1]![](assets/理解大语言模型/file-20260228203251056.png)
1. 创建 LLM 的第一步是用大量文本数据进行训练，这些数据一般被称为原始文本。这里的 "raw" 指的是这些数据只是普通文本，没有任何标注信息。这一步被称之为**预训练**，旨在创建一个初始的预训练 LLM，通常称为基础模型。
2. 该LLM会学习预测文本中的下一个单词。我们可以在优质的标注数据上对 LLM 进行进一步训练，这个过程称为微调。
	- 指令微调:标注数据集包含指令和答案对，例如用于翻译文本的查询及其正确翻译
	- 分类任务微调:比如与垃圾邮件和非垃圾邮件标签相关的电子邮件。


## Transformer 架构

大多数现代 LLM 基于 transformer 架构，这是一种深度神经网络架构，首次在 2017 年的论文《Attention Is All You Need》中提出。为了理解 LLM，我们需要简要回顾一下最初为机器翻译开发的原始 Transformer
![](assets/理解大语言模型/file-20260228204029117.png)

Transformer 架构由两个子模块组成
- 编码器:编码器模块处理文本输入，将其编码为一系列数值表示或向量，以捕捉输入的上下文信息
- 解码器:解码器模块利用这些编码向量生成输出文本
编码器和解码器都由多个层通过自注意力机制相连[^2]

==BERT 是基于原始 Transformer 架构的**编码器**子模块==，与 GPT 的训练方法有所不同。GPT 主要用于生成任务，而 BERT 及其变体则专注于掩码词预测，即模型在给定句子中预测被掩码或隐藏的词。这种独特的训练策略使得 BERT 在文本分类任务中具备优势，包括情感预测和文档分类。

==GPT 专注于原始 Transformer 架构中的解码器部分==，被设计用于需要生成文本的任务。这些模型擅长执行zero-shot 和few-shot 学习任务
 - zero-shot 学习指的是在没有先前具体示例的情况下，能够处理完全未见过的任务。
 - few-shot 学习则是指模型可以从用户提供的极少量示例中进行学习

![](assets/理解大语言模型/file-20260228204612326.png)
![](assets/理解大语言模型/file-20260228205104145.png)

==并非所有的 Transformers 都是 LLM，因为它们也可以用于计算机视觉。同时，并非所有的 LLM 都是基于 Transformers 的，市场上也有一些基于递归和卷积架构的大语言模型==

## 大型数据集

![](assets/理解大语言模型/file-20260228233303201.png)

流行的 GPT 和 BERT 类模型的大型训练数据集代表了丰富而全面的文本语料库，涵盖数十亿个单词，涉及各种主题以及自然语言和计算机语言。这个训练数据集的规模和多样性使得这些模型在各种任务中表现优异，包括不同语言的语法、语义和上下文信息，甚至还可以处理一些需要通用知识的任务。

## GPT架构



[用于 LLM 预训练研究的三万亿token开放语料库](https://arxiv.org/abs/2402.00159)

[^1]: 预训练的数据集已经学习好了语言模型的基础能力。
	而微调则是利用特定领域的数据来让模型适应某些特定的任务。
		全权重的微调
		冻结部分权重的微调：冻结部分权重的微调

[^2]: 早期用于翻译任务的模型一般使用RNN，RNN的核心是循环结构，也就是会把当前的输出和之前的状态结合起来，再输入到下一步。
	这种机制也有一个非常明显的不足：长距离依赖问题,它对非常长的序列记忆能力有限。
	Transformer架构通过自注意力机制实现能够关注序列中的任意位置，而不需要经过层层传递。
